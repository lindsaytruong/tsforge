{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ec8dcf",
   "metadata": {},
   "source": [
    "# Welcome to Forecast Acadeny - Forecasting @ Scale\n",
    "## 00 - Initial Setup\n",
    "### What weâ€™ll do\n",
    "\n",
    "**Goal:** get your machine ready, download the M5 dataset via Nixtla, create a small teaching subset, and verify everything with a quick diagnostic.\n",
    "Youâ€™ll do this once, then reuse the outputs in later lessons.\n",
    "In this setup notebook we will:\n",
    "\n",
    "1. **Create the course folder structure**  \n",
    "   Organize inputs, outputs, and interim data so everything is easy to find.\n",
    "\n",
    "2. **(Optional) Install/verify dependencies**  \n",
    "   Make sure you have `datasetsforecast`, `pyarrow`, `tsforge`, and other packages ready.\n",
    "\n",
    "3. **Download the M5 dataset**  \n",
    "   Use Nixtlaâ€™s `datasetsforecast` loader to fetch *sales*, *calendar*, and *prices* data.\n",
    "\n",
    "4. **Save raw data to `data/input/raw/`**  \n",
    "   Store the full files as Parquet for faster loading and smaller size.\n",
    "\n",
    "5. **Build a teaching subset**  \n",
    "   Use `tsforge` to create a smaller sample (few departments/stores/items) and save it to `data/input/processed/`.\n",
    "\n",
    "6. **Run a quick diagnostic**  \n",
    "   Check the subset for completeness (no missing periods) so we know itâ€™s healthy for forecasting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c608e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created/verified folders:\n",
      " - c:\\Users\\tacke\\Documents\\GitHub\\tsforge\\data\\input\\raw\n",
      " - c:\\Users\\tacke\\Documents\\GitHub\\tsforge\\data\\input\\processed\n",
      " - c:\\Users\\tacke\\Documents\\GitHub\\tsforge\\data\\interim\n",
      " - c:\\Users\\tacke\\Documents\\GitHub\\tsforge\\data\\output\n",
      " - c:\\Users\\tacke\\Documents\\GitHub\\tsforge\\data\\output\\models\n",
      " - c:\\Users\\tacke\\Documents\\GitHub\\tsforge\\data\\output\\forecasts\n",
      " - c:\\Users\\tacke\\Documents\\GitHub\\tsforge\\data\\output\\diagnostics\n",
      " - c:\\Users\\tacke\\Documents\\GitHub\\tsforge\\data\\output\\plots\n",
      " - c:\\Users\\tacke\\Documents\\GitHub\\tsforge\\docs\\figures\n"
     ]
    }
   ],
   "source": [
    "## Create Project Paths and Folders\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# notebook root\n",
    "ROOT = Path.cwd()\n",
    "\n",
    "# one folder up (..)\n",
    "BASE = ROOT.parent\n",
    "\n",
    "DATA_DIR = BASE / \"data\"\n",
    "INPUT_RAW = DATA_DIR / \"input\" / \"raw\"\n",
    "INPUT_PROCESSED = DATA_DIR / \"input\" / \"processed\"\n",
    "INTERIM_DIR = DATA_DIR / \"interim\"\n",
    "OUTPUT_DIR = DATA_DIR / \"output\"\n",
    "OUTPUT_MODELS = OUTPUT_DIR / \"models\"\n",
    "OUTPUT_FORECASTS = OUTPUT_DIR / \"forecasts\"\n",
    "OUTPUT_DIAG = OUTPUT_DIR / \"diagnostics\"\n",
    "OUTPUT_PLOTS = OUTPUT_DIR / \"plots\"\n",
    "DOCS_FIGS = BASE / \"docs\" / \"figures\"\n",
    "\n",
    "for p in [\n",
    "    INPUT_RAW, INPUT_PROCESSED, INTERIM_DIR,\n",
    "    OUTPUT_DIR, OUTPUT_MODELS, OUTPUT_FORECASTS, OUTPUT_DIAG, OUTPUT_PLOTS,\n",
    "    DOCS_FIGS\n",
    "]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Created/verified folders:\")\n",
    "for p in [    INPUT_RAW, INPUT_PROCESSED, INTERIM_DIR,\n",
    "    OUTPUT_DIR, OUTPUT_MODELS, OUTPUT_FORECASTS, OUTPUT_DIAG, OUTPUT_PLOTS,\n",
    "    DOCS_FIGS]:\n",
    "    print(\" -\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034f2904",
   "metadata": {},
   "source": [
    "### 2) (Optional) installs\n",
    "\n",
    "If you didnâ€™t install these in your VS Code environment already, uncomment and run:\n",
    "\n",
    "- **datasetsforecast** â†’ for the official M5 loader (Nixtla)  \n",
    "- **pyarrow** â†’ for fast Parquet I/O  \n",
    "- **tsforge** â†’ your package with teaching utilities  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "110ed8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U datasetsforecast pyarrow\n",
    "# !pip install -U tsforge          # if published, or `pip install -e .` from your tsforge repo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d09002",
   "metadata": {},
   "source": [
    "### 3) Imports & Version Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65a8d02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.10 | OS: Windows\n",
      "pandas: 2.3.2\n"
     ]
    }
   ],
   "source": [
    "import sys, platform\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0], \"| OS:\", platform.system())\n",
    "print(\"pandas:\", pd.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24519df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43da259",
   "metadata": {},
   "source": [
    "### 4) Download M5 via Nixtla and save raw to `data/input/raw/`\n",
    "\n",
    "Weâ€™ll use Nixtlaâ€™s official loader (`datasetsforecast.m5.M5`) which downloads & caches M5 locally.  \n",
    "We then save what we loaded as **Parquet files** in `data/input/raw/` so later notebooks donâ€™t need to refetch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "405f34a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50.2M/50.2M [00:00<00:00, 67.6MiB/s]\n",
      "INFO:datasetsforecast.utils:Successfully downloaded m5.zip, 50219189, bytes.\n",
      "INFO:datasetsforecast.utils:Decompressing zip file...\n",
      "INFO:datasetsforecast.utils:Successfully decompressed ..\\data\\input\\raw\\m5\\datasets\\m5.zip\n",
      "c:\\Users\\tacke\\Documents\\GitHub\\tsforge\\.venv\\Lib\\site-packages\\datasetsforecast\\m5.py:143: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  keep_mask = long.groupby('id')['y'].transform(first_nz_mask, engine='numba')\n",
      "c:\\Users\\tacke\\Documents\\GitHub\\tsforge\\.venv\\Lib\\site-packages\\datasetsforecast\\m5.py:145: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  long.rename(columns={'id': 'unique_id', 'date': 'ds'}, inplace=True)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50.2M/50.2M [00:01<00:00, 48.3MiB/s]\n",
      "INFO:datasetsforecast.utils:Successfully downloaded m5.zip, 50219189, bytes.\n",
      "INFO:datasetsforecast.utils:Decompressing zip file...\n",
      "INFO:datasetsforecast.utils:Successfully decompressed ..\\data\\input\\raw\\nixtla_cache\\m5\\datasets\\m5.zip\n",
      "c:\\Users\\tacke\\Documents\\GitHub\\tsforge\\.venv\\Lib\\site-packages\\datasetsforecast\\m5.py:143: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  keep_mask = long.groupby('id')['y'].transform(first_nz_mask, engine='numba')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved clean M5 files without Nixtla unique_id:\n",
      " - 00_m5_sales.parquet (39.1 MB)\n",
      " - 00_m5_calendar.parquet (0.0 MB)\n",
      " - 00_m5_prices.parquet (57.0 MB)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datasetsforecast.m5 import M5\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Paths\n",
    "# -------------------------------------------------------------------\n",
    "# Go directly to data/raw\n",
    "INPUT_RAW = Path(\"..\") / \"data\" / \"input\" / \"raw\"\n",
    "INPUT_RAW.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Tell Nixtla to build cache here\n",
    "Y_df, X_df, meta_df = M5.load(directory=str(INPUT_RAW), cache=True)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Load with Nixtla\n",
    "# This gives us 3 dfs: Y_df (sales), X_df (calendar+snap+prices), meta_df (meta)\n",
    "# -------------------------------------------------------------------\n",
    "Y_df, X_df, meta_df = M5.load(directory=str(INPUT_RAW / \"nixtla_cache\"), cache=True)\n",
    "\n",
    "# Messy up the data a bit to simulate real data\n",
    "meta_df[['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']] = meta_df[['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].astype(str)\n",
    "sales_df = Y_df.merge(meta_df, on=['unique_id'], how=\"left\")\n",
    "sales_df = sales_df.rename(columns={\"ds\": \"date\", \"y\": \"sales\"})\n",
    "sales_df = sales_df[['item_id','dept_id','cat_id','store_id','state_id','date','sales']]\n",
    "sales_df = sales_df[sales_df.sales>0]  # remove zero sales rows\n",
    "\n",
    "prices_df = X_df.merge(meta_df, on=['unique_id'], how=\"left\")\n",
    "prices_df = prices_df.rename(columns={\"ds\": \"date\", \"sell_price\": \"price\"})\n",
    "calendar_df = prices_df[['date','event_name_1','event_type_1','event_name_2','event_type_2','snap_CA','snap_TX','snap_WI']].drop_duplicates()\n",
    "\n",
    "prices_df = prices_df[['item_id','store_id','date','price']]\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Save parquet outputs\n",
    "# -------------------------------------------------------------------\n",
    "sales_path = INPUT_RAW / \"00_m5_sales.parquet\"\n",
    "calendar_path = INPUT_RAW / \"00_m5_calendar.parquet\"\n",
    "prices_path = INPUT_RAW / \"00_m5_prices.parquet\"\n",
    "#meta_path = INPUT_RAW / \"00_m5_meta.parquet\"\n",
    "\n",
    "sales_df.to_parquet(sales_path, index=False)\n",
    "calendar_df.to_parquet(calendar_path, index=False)\n",
    "prices_df.to_parquet(prices_path, index=False)\n",
    "\n",
    "print(\"âœ… Saved clean M5 files without Nixtla unique_id:\")\n",
    "print(\" -\", sales_path.name, f\"({round(sales_path.stat().st_size/1e6,1)} MB)\")\n",
    "print(\" -\", calendar_path.name, f\"({round(calendar_path.stat().st_size/1e6,1)} MB)\")\n",
    "print(\" -\", prices_path.name, f\"({round(prices_path.stat().st_size/1e6,1)} MB)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b547e90",
   "metadata": {},
   "source": [
    "The sales data we just downloaded is the official Kaggle M5 training set (sales_train_validation.csv in wide format). It runs ~1913 days, ending 2016-06-19. Kaggleâ€™s private leaderboard was scored on a hidden 28-day test horizon, but for our learning weâ€™ll create our own validation splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559852b2",
   "metadata": {},
   "source": [
    "#### Examine Sales Data\n",
    "Here we will examine the sales data.  \n",
    "The unique identifier in this data is item_id and store_id.  There are actually 2 hierarchies here, product and location. \n",
    "\n",
    "**Product Hierarchy:** \n",
    "An item belongs to a department which belongs to a  category, so our product hierachy is item -> department -> category.\n",
    "\n",
    "**Location Hierarchy**\n",
    "A store belongs to a state, so our hierarchy is store -> state.\n",
    "\n",
    "When modeling, it is helpful to have a `unique_id` that we can reference easily when merging data or modeling, so lets create 2 things.\n",
    "1. A `unique_id` field in our dataset.\n",
    "2. A dataframe called  `meta_df` which stores the hierarchy information for our `unique_id`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e6edf38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>date</th>\n",
       "      <th>sales</th>\n",
       "      <th>unique_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>FOODS_1</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>3.0</td>\n",
       "      <td>FOODS_1_001_CA_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>FOODS_1</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2011-02-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FOODS_1_001_CA_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>FOODS_1</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2011-02-02</td>\n",
       "      <td>4.0</td>\n",
       "      <td>FOODS_1_001_CA_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>FOODS_1</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2011-02-03</td>\n",
       "      <td>2.0</td>\n",
       "      <td>FOODS_1_001_CA_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>FOODS_1</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2011-02-05</td>\n",
       "      <td>2.0</td>\n",
       "      <td>FOODS_1_001_CA_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       item_id  dept_id cat_id store_id state_id       date  sales  \\\n",
       "0  FOODS_1_001  FOODS_1  FOODS     CA_1       CA 2011-01-29    3.0   \n",
       "1  FOODS_1_001  FOODS_1  FOODS     CA_1       CA 2011-02-01    1.0   \n",
       "2  FOODS_1_001  FOODS_1  FOODS     CA_1       CA 2011-02-02    4.0   \n",
       "3  FOODS_1_001  FOODS_1  FOODS     CA_1       CA 2011-02-03    2.0   \n",
       "4  FOODS_1_001  FOODS_1  FOODS     CA_1       CA 2011-02-05    2.0   \n",
       "\n",
       "          unique_id  \n",
       "0  FOODS_1_001_CA_1  \n",
       "1  FOODS_1_001_CA_1  \n",
       "2  FOODS_1_001_CA_1  \n",
       "3  FOODS_1_001_CA_1  \n",
       "4  FOODS_1_001_CA_1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_df = pd.read_parquet(r'C:\\Users\\tacke\\Documents\\GitHub\\forecast_academy\\data\\input\\raw\\00_m5_sales.parquet')\n",
    "sales_df['unique_id'] = sales_df['item_id'] + '_' + sales_df['store_id'] \n",
    "sales_df.to_parquet('../data/input/processed/00_m5_sales_full.parquet')\n",
    "sales_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33a894f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FOODS_1_001_CA_1</td>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>FOODS_1</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>FOODS_1_001_CA_2</td>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>FOODS_1</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>CA_2</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1881</th>\n",
       "      <td>FOODS_1_001_CA_3</td>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>FOODS_1</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2771</th>\n",
       "      <td>FOODS_1_001_CA_4</td>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>FOODS_1</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>CA_4</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3281</th>\n",
       "      <td>FOODS_1_001_TX_1</td>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>FOODS_1</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             unique_id      item_id  dept_id cat_id store_id state_id\n",
       "0     FOODS_1_001_CA_1  FOODS_1_001  FOODS_1  FOODS     CA_1       CA\n",
       "864   FOODS_1_001_CA_2  FOODS_1_001  FOODS_1  FOODS     CA_2       CA\n",
       "1881  FOODS_1_001_CA_3  FOODS_1_001  FOODS_1  FOODS     CA_3       CA\n",
       "2771  FOODS_1_001_CA_4  FOODS_1_001  FOODS_1  FOODS     CA_4       CA\n",
       "3281  FOODS_1_001_TX_1  FOODS_1_001  FOODS_1  FOODS     TX_1       TX"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_df = sales_df[['unique_id','item_id','dept_id','cat_id','store_id','state_id']].drop_duplicates()\n",
    "meta_df.to_parquet('../data/input/processed/00_m5_meta_full.parquet')\n",
    "\n",
    "meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "366bac42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>unique_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>2.0</td>\n",
       "      <td>FOODS_1_001_CA_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>2011-01-30</td>\n",
       "      <td>2.0</td>\n",
       "      <td>FOODS_1_001_CA_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>2011-01-31</td>\n",
       "      <td>2.0</td>\n",
       "      <td>FOODS_1_001_CA_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>2011-02-01</td>\n",
       "      <td>2.0</td>\n",
       "      <td>FOODS_1_001_CA_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>2011-02-02</td>\n",
       "      <td>2.0</td>\n",
       "      <td>FOODS_1_001_CA_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       item_id store_id       date  price         unique_id\n",
       "0  FOODS_1_001     CA_1 2011-01-29    2.0  FOODS_1_001_CA_1\n",
       "1  FOODS_1_001     CA_1 2011-01-30    2.0  FOODS_1_001_CA_1\n",
       "2  FOODS_1_001     CA_1 2011-01-31    2.0  FOODS_1_001_CA_1\n",
       "3  FOODS_1_001     CA_1 2011-02-01    2.0  FOODS_1_001_CA_1\n",
       "4  FOODS_1_001     CA_1 2011-02-02    2.0  FOODS_1_001_CA_1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_df = pd.read_parquet(r'C:\\Users\\tacke\\Documents\\GitHub\\forecast_academy\\data\\input\\raw\\00_m5_prices.parquet')\n",
    "prices_df['unique_id'] = prices_df['item_id'] + '_' + prices_df['store_id'] \n",
    "prices_df.to_parquet('../data/input/processed/00_m5_prices_full.parquet')\n",
    "prices_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c1e0e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tacke\\AppData\\Local\\Temp\\ipykernel_32300\\661034389.py:2: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n",
      "  calendar_df.replace('nan', pd.NA, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-02-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date event_name_1 event_type_1 event_name_2 event_type_2  snap_CA  \\\n",
       "0 2011-01-29          NaN          NaN          NaN          NaN        0   \n",
       "1 2011-01-30          NaN          NaN          NaN          NaN        0   \n",
       "2 2011-01-31          NaN          NaN          NaN          NaN        0   \n",
       "3 2011-02-01          NaN          NaN          NaN          NaN        1   \n",
       "4 2011-02-02          NaN          NaN          NaN          NaN        1   \n",
       "\n",
       "   snap_TX  snap_WI  \n",
       "0        0        0  \n",
       "1        0        0  \n",
       "2        0        0  \n",
       "3        1        0  \n",
       "4        0        1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar_df = pd.read_parquet(r'C:\\Users\\tacke\\Documents\\GitHub\\forecast_academy\\data\\input\\raw\\00_m5_calendar.parquet')\n",
    "calendar_df.replace('nan', pd.NA, inplace=True)\n",
    "calendar_df.to_parquet('../data/input/processed/00_m5_calendar.parquet')\n",
    "calendar_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cafc45f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19321177, 8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8325c1a6",
   "metadata": {},
   "source": [
    "### 5) Create a small teaching subset and save to `data/input/processed/`\n",
    "\n",
    "Here is a challenge with m5, its over 19M rows of data.  While its not a deal breaker, for the purposes of this training, it is not necessary to forecast the entire population as it will just take unnecessary processing time. \n",
    "For this training, let's subset the data so it is easier to work with.  However, if you want to explore how things work on the full dataset, simply choose `subset=False` in the next block.\n",
    "\n",
    "We will limit our subset to **FOODS** and **HOUSEHOLD** categor items, in the stores **CA_1 + TX_2 + TX_1**.\n",
    "This keeps lessons fast while preserving hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15e7e4b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['FOODS', 'HOBBIES', 'HOUSEHOLD'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_df['cat_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8073e83d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1047"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_df[sales_df['cat_id']=='HOUSEHOLD'].item_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ab28c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1',\n",
       "       'WI_2', 'WI_3'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_df['store_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9e04a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2011-01-29 00:00:00')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_df.date.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16d6786b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2016-06-19 00:00:00')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_df.date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45268ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting to 2170376 rows of data\n"
     ]
    }
   ],
   "source": [
    "subset = True  # set to False to use full data (19M rows)\n",
    "if subset:\n",
    "    sales_df_sub = sales_df[sales_df['store_id'].isin(['CA_1','CA_2','TX_1']) & sales_df['cat_id'].isin(['HOBBIES','HOUSEHOLD'])]\n",
    "    sales_df_sub = sales_df_sub[sales_df_sub['date']>='2012-06-19']\n",
    "    unique_ids = sales_df_sub['unique_id'].unique()\n",
    "    prices_df_sub = prices_df[prices_df.unique_id.isin(unique_ids)]\n",
    "    meta_df_sub = prices_df[prices_df.unique_id.isin(unique_ids)]\n",
    "    print(f\"Subsetting to {sales_df_sub.shape[0]} rows of data\")\n",
    "\n",
    "sales_df_sub.to_parquet('../data/input/processed/00_m5_sales_subset.parquet')\n",
    "meta_df_sub.to_parquet('../data/input/processed/00_m5_meta_subset.parquet')\n",
    "prices_df_sub.to_parquet('../data/input/processed/00_m5_prices_subset.parquet') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6328db08",
   "metadata": {},
   "source": [
    "### 6) Make Training and Test Set\n",
    "\n",
    "Because we do not want any cheating and make this as realistic as possible, we want to immediately split off a test set so even during the EDA process, we do not get any peaks in to the future data.\n",
    "To be consistent with the M5, we will pull out 28 days at the end of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8e6a3aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2016-06-19 00:00:00')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_df_sub.date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94b159a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training/test sets saved:\n"
     ]
    }
   ],
   "source": [
    "def make_train_test_split(df: pd.DataFrame, date_col: str = \"date\", horizon: int = 28):\n",
    "    \"\"\"\n",
    "    Split a long-format time series panel into train/test sets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Must have a datetime column named `date_col`.\n",
    "    date_col : str, default \"date\"\n",
    "        Name of datetime column.\n",
    "    horizon : int, default 28\n",
    "        Forecast horizon length (days).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_df, test_df : tuple of DataFrames\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    max_date = df[date_col].max()\n",
    "    cutoff = max_date - pd.Timedelta(days=horizon)\n",
    "    train = df[df[date_col] <= cutoff]\n",
    "    test = df[df[date_col] > cutoff]\n",
    "    return train, test\n",
    "\n",
    "INPUT_PROCESSED = Path(\"..\") / \"data\" / \"input\" / \"processed\"\n",
    "\n",
    "# Train/test splits\n",
    "train_df, test_df = make_train_test_split(sales_df_sub, date_col=\"date\", horizon=28)\n",
    "\n",
    "# Save outputs\n",
    "train_df.to_parquet(INPUT_PROCESSED / \"00_m5_sales_train.parquet\", index=False)\n",
    "test_df.to_parquet(INPUT_PROCESSED / \"00_m5_sales_test.parquet\", index=False)\n",
    "\n",
    "print(\"âœ… Training/test sets saved:\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f8cd5e",
   "metadata": {},
   "source": [
    "### 7) Quick sanity checks\n",
    "\n",
    "Weâ€™ll run a few checks to make sure the train/test splits are valid:\n",
    "\n",
    "- **Check consistency between train and test**:  \n",
    "  - Train end date and test start date are exactly 1 day apart  \n",
    "  - All series IDs in test are also present in train  \n",
    "  - The number of unique series is consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "deb2cdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset TRAIN rows: 2116761 | Date range: 2012-06-19 00:00:00 â†’ 2016-05-22 00:00:00\n",
      "Subset TEST rows: 53615 | Date range: 2016-05-23 00:00:00 â†’ 2016-06-19 00:00:00\n",
      "\n",
      "Consistency checks:\n",
      " - Train last date: 2016-05-22 00:00:00\n",
      " - Test first date: 2016-05-23 00:00:00\n",
      " - Gap between train end and test start: 1 days\n",
      " - All test IDs in train? True\n",
      " - # unique IDs in train: 4836 | in test: 4836\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------\n",
    "# Quick info\n",
    "print(\"Subset TRAIN rows:\", len(train_df), \n",
    "      \"| Date range:\", train_df[\"date\"].min(), \"â†’\", train_df[\"date\"].max())\n",
    "print(\"Subset TEST rows:\", len(test_df), \n",
    "      \"| Date range:\", test_df[\"date\"].min(), \"â†’\", test_df[\"date\"].max())\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Consistency checks\n",
    "train_end = train_df[\"date\"].max()\n",
    "test_start = test_df[\"date\"].min()\n",
    "\n",
    "ids_train = set(train_df[\"unique_id\"].unique())\n",
    "ids_test = set(test_df[\"unique_id\"].unique())\n",
    "\n",
    "print(\"\\nConsistency checks:\")\n",
    "print(\" - Train last date:\", train_end)\n",
    "print(\" - Test first date:\", test_start)\n",
    "print(\" - Gap between train end and test start:\", (test_start - train_end).days, \"days\")\n",
    "print(\" - All test IDs in train?\", ids_test.issubset(ids_train))\n",
    "print(\" - # unique IDs in train:\", len(ids_train), \"| in test:\", len(ids_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec00ccc",
   "metadata": {},
   "source": [
    "### 7) (Optional) .gitignore helper\n",
    "\n",
    "If youâ€™re versioning this repo, itâ€™s smart to ignore **raw data** and **outputs**.  \n",
    "Weâ€™ll generate a starter `.gitignore` so large files donâ€™t accidentally end up in git history. Run this cell once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "307ad5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created .gitignore with data folders ignored.\n"
     ]
    }
   ],
   "source": [
    "gitignore_path = ROOT / \".gitignore\"\n",
    "lines = [\n",
    "    \"# data (raw, interim, output)\",\n",
    "    \"data/input/raw/\",\n",
    "    \"data/interim/\",\n",
    "    \"data/output/\",\n",
    "    \"\",\n",
    "    \"# OS/editor files\",\n",
    "    \".DS_Store\",\n",
    "    \".ipynb_checkpoints/\",\n",
    "    \"\"\n",
    "]\n",
    "\n",
    "if not gitignore_path.exists():\n",
    "    gitignore_path.write_text(\"\\n\".join(lines))\n",
    "    print(\"Created .gitignore with data folders ignored.\")\n",
    "else:\n",
    "    print(\".gitignore already exists â€” review to ensure data folders are ignored.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e46bb70",
   "metadata": {},
   "source": [
    "### Youâ€™re set ðŸŽ‰\n",
    "\n",
    "- **Raw M5** lives in `data/input/raw/` (Parquet).  \n",
    "- **Teaching subset** lives in `data/input/processed/`.  \n",
    "\n",
    "**Next:** move to `01_initial_eda.ipynb` to define the forecast charter (target, grain, horizon, metrics) and start exploring the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddc7c32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
