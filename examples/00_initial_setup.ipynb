{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ec8dcf",
   "metadata": {},
   "source": [
    "# Welcome to Forecast Acadeny - Forecasting @ Scale\n",
    "## 00 - Initial Setup\n",
    "### What weâ€™ll do\n",
    "\n",
    "**Goal:** get your machine ready, download the M5 dataset via Nixtla, create a small teaching subset, and verify everything with a quick diagnostic.\n",
    "Youâ€™ll do this once, then reuse the outputs in later lessons.\n",
    "In this setup notebook we will:\n",
    "\n",
    "1. **Create the course folder structure**  \n",
    "   Organize inputs, outputs, and interim data so everything is easy to find.\n",
    "\n",
    "2. **(Optional) Install/verify dependencies**  \n",
    "   Make sure you have `datasetsforecast`, `pyarrow`, `tsforge`, and other packages ready.\n",
    "\n",
    "3. **Download the M5 dataset**  \n",
    "   Use Nixtlaâ€™s `datasetsforecast` loader to fetch *sales*, *calendar*, and *prices* data.\n",
    "\n",
    "4. **Save raw data to `data/input/raw/`**  \n",
    "   Store the full files as Parquet for faster loading and smaller size.\n",
    "\n",
    "5. **Build a teaching subset**  \n",
    "   Use `tsforge` to create a smaller sample (few departments/stores/items) and save it to `data/input/processed/`.\n",
    "\n",
    "6. **Run a quick diagnostic**  \n",
    "   Check the subset for completeness (no missing periods) so we know itâ€™s healthy for forecasting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c608e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created/verified folders:\n",
      " - c:\\Users\\tacke\\Documents\\GitHub\\forecast_academy\\data\\input\\raw\n",
      " - c:\\Users\\tacke\\Documents\\GitHub\\forecast_academy\\data\\input\\processed\n",
      " - c:\\Users\\tacke\\Documents\\GitHub\\forecast_academy\\data\\interim\n",
      " - c:\\Users\\tacke\\Documents\\GitHub\\forecast_academy\\data\\output\n",
      " - c:\\Users\\tacke\\Documents\\GitHub\\forecast_academy\\data\\output\\models\n",
      " - c:\\Users\\tacke\\Documents\\GitHub\\forecast_academy\\data\\output\\forecasts\n",
      " - c:\\Users\\tacke\\Documents\\GitHub\\forecast_academy\\data\\output\\diagnostics\n",
      " - c:\\Users\\tacke\\Documents\\GitHub\\forecast_academy\\data\\output\\plots\n",
      " - c:\\Users\\tacke\\Documents\\GitHub\\forecast_academy\\docs\\figures\n"
     ]
    }
   ],
   "source": [
    "## Create Project Paths and Folders\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# notebook root\n",
    "ROOT = Path.cwd()\n",
    "\n",
    "# one folder up (..)\n",
    "BASE = ROOT.parent\n",
    "\n",
    "DATA_DIR = BASE / \"data\"\n",
    "INPUT_RAW = DATA_DIR / \"input\" / \"raw\"\n",
    "INPUT_PROCESSED = DATA_DIR / \"input\" / \"processed\"\n",
    "INTERIM_DIR = DATA_DIR / \"interim\"\n",
    "OUTPUT_DIR = DATA_DIR / \"output\"\n",
    "OUTPUT_MODELS = OUTPUT_DIR / \"models\"\n",
    "OUTPUT_FORECASTS = OUTPUT_DIR / \"forecasts\"\n",
    "OUTPUT_DIAG = OUTPUT_DIR / \"diagnostics\"\n",
    "OUTPUT_PLOTS = OUTPUT_DIR / \"plots\"\n",
    "DOCS_FIGS = BASE / \"docs\" / \"figures\"\n",
    "\n",
    "for p in [\n",
    "    INPUT_RAW, INPUT_PROCESSED, INTERIM_DIR,\n",
    "    OUTPUT_DIR, OUTPUT_MODELS, OUTPUT_FORECASTS, OUTPUT_DIAG, OUTPUT_PLOTS,\n",
    "    DOCS_FIGS\n",
    "]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Created/verified folders:\")\n",
    "for p in [    INPUT_RAW, INPUT_PROCESSED, INTERIM_DIR,\n",
    "    OUTPUT_DIR, OUTPUT_MODELS, OUTPUT_FORECASTS, OUTPUT_DIAG, OUTPUT_PLOTS,\n",
    "    DOCS_FIGS]:\n",
    "    print(\" -\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034f2904",
   "metadata": {},
   "source": [
    "### 2) (Optional) installs\n",
    "\n",
    "If you didnâ€™t install these in your VS Code environment already, uncomment and run:\n",
    "\n",
    "- **datasetsforecast** â†’ for the official M5 loader (Nixtla)  \n",
    "- **pyarrow** â†’ for fast Parquet I/O  \n",
    "- **tsforge** â†’ your package with teaching utilities  \n",
    "- **pytimetk** â†’ time-series EDA helpers (used later in the course)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110ed8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U datasetsforecast pyarrow\n",
    "# !pip install -U tsforge          # if published, or `pip install -e .` from your tsforge repo\n",
    "# !pip install -U pytimetk         # optional: used in later EDA/feature lessons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d09002",
   "metadata": {},
   "source": [
    "### 3) Imports & Version Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65a8d02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.7 | OS: Windows\n",
      "pandas: 2.3.2\n"
     ]
    }
   ],
   "source": [
    "import sys, platform\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0], \"| OS:\", platform.system())\n",
    "print(\"pandas:\", pd.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43da259",
   "metadata": {},
   "source": [
    "### 4) Download M5 via Nixtla and save raw to `data/input/raw/`\n",
    "\n",
    "Weâ€™ll use Nixtlaâ€™s official loader (`datasetsforecast.m5.M5`) which downloads & caches M5 locally.  \n",
    "We then save what we loaded as **Parquet files** in `data/input/raw/` so later notebooks donâ€™t need to refetch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "405f34a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50.2M/50.2M [00:01<00:00, 46.6MiB/s]\n",
      "INFO:datasetsforecast.utils:Successfully downloaded m5.zip, 50219189, bytes.\n",
      "INFO:datasetsforecast.utils:Decompressing zip file...\n",
      "INFO:datasetsforecast.utils:Successfully decompressed c:\\Users\\tacke\\Documents\\GitHub\\forecast_academy\\data\\input\\raw\\nixtla_cache\\m5\\datasets\\m5.zip\n",
      "c:\\Users\\tacke\\Documents\\GitHub\\tsforge\\.venv\\Lib\\site-packages\\datasetsforecast\\m5.py:132: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  without_leading_zeros = long['y'].gt(0).groupby(long['id']).transform('cummax')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned raw files:\n",
      " - m5_sales.parquet (85.0 MB)\n",
      " - m5_calendar.parquet (67.2 MB)\n",
      " - m5_prices.parquet (0.2 MB)\n"
     ]
    }
   ],
   "source": [
    "from datasetsforecast.m5 import M5\n",
    "\n",
    "RAW_CACHE = str(INPUT_RAW / \"nixtla_cache\")  # keep nixtla internals in a subfolder\n",
    "\n",
    "# Download using Nixtla\n",
    "Y_df, X_df, prices_df = M5.load(directory=RAW_CACHE, cache=True)\n",
    "\n",
    "# Rename columns for teaching\n",
    "sales_df = Y_df.rename(columns={\"unique_id\": \"id\", \"ds\": \"date\", \"y\": \"sales\"})\n",
    "calendar_df = X_df.rename(columns={\"ds\": \"date\"})\n",
    "sales_df[\"date\"] = pd.to_datetime(sales_df[\"date\"])\n",
    "calendar_df[\"date\"] = pd.to_datetime(calendar_df[\"date\"])\n",
    "\n",
    "# Save flat Parquet copies in data/input/raw/\n",
    "sales_path = INPUT_RAW / \"m5_sales.parquet\"\n",
    "calendar_path = INPUT_RAW / \"m5_calendar.parquet\"\n",
    "prices_path = INPUT_RAW / \"m5_prices.parquet\"\n",
    "\n",
    "sales_df.to_parquet(sales_path, index=False)\n",
    "calendar_df.to_parquet(calendar_path, index=False)\n",
    "prices_df.to_parquet(prices_path, index=False)\n",
    "\n",
    "print(\"Saved cleaned raw files:\")\n",
    "for p in [sales_path, calendar_path, prices_path]:\n",
    "    print(\" -\", p.relative_to(INPUT_RAW), f\"({round(p.stat().st_size/1e6, 1)} MB)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668d3630",
   "metadata": {},
   "source": [
    "The sales data we just downloaded is the official Kaggle M5 training set (sales_train_validation.csv in wide format). It runs ~1913 days, ending 2016-06-19. Kaggleâ€™s private leaderboard was scored on a hidden 28-day test horizon, but for our learning weâ€™ll create our own validation splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fbb092",
   "metadata": {},
   "source": [
    "### 5) Create a small teaching subset and save to `data/input/processed/`\n",
    "\n",
    "Weâ€™ll use a helper that returns a clean slice (e.g., **FOODS_3 + HOUSEHOLD_1**, stores **CA_1 + TX_1**, ~5 items per store/department).  \n",
    "This keeps lessons fast while preserving hierarchy.\n",
    "\n",
    "If your `tsforge` function is available (`tsforge.datasets.load_m5_subset`), weâ€™ll use it.  \n",
    "Otherwise, a small fallback function does the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57243ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('data/input/raw')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(\"..\") / \"data\" / \"input\" / \"raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94b159a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training/test sets saved:\n",
      " - Sales (full + subset)\n",
      " - Calendar (full only)\n",
      " - Prices (full only)\n"
     ]
    }
   ],
   "source": [
    "def make_train_test_split(df: pd.DataFrame, date_col: str = \"date\", horizon: int = 28):\n",
    "    \"\"\"\n",
    "    Split a long-format time series panel into train/test sets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Must have a datetime column named `date_col`.\n",
    "    date_col : str, default \"date\"\n",
    "        Name of datetime column.\n",
    "    horizon : int, default 28\n",
    "        Forecast horizon length (days).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_df, test_df : tuple of DataFrames\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    max_date = df[date_col].max()\n",
    "    cutoff = max_date - pd.Timedelta(days=horizon)\n",
    "    train = df[df[date_col] <= cutoff]\n",
    "    test = df[df[date_col] > cutoff]\n",
    "    return train, test\n",
    "\n",
    "\n",
    "INPUT_RAW = Path(\"..\") / \"data\" / \"input\" / \"raw\"\n",
    "INPUT_PROCESSED = Path(\"..\") / \"data\" / \"input\" / \"processed\"\n",
    "\n",
    "# Load raw parquet (already standardized in setup)\n",
    "sales = pd.read_parquet(INPUT_RAW / \"m5_sales.parquet\")\n",
    "calendar = pd.read_parquet(INPUT_RAW / \"m5_calendar.parquet\")\n",
    "prices = pd.read_parquet(INPUT_RAW / \"m5_prices.parquet\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Build subset (teaching slice)\n",
    "depts = [\"FOODS_3\", \"HOUSEHOLD_1\"]\n",
    "stores = [\"CA_1\", \"TX_1\"]\n",
    "\n",
    "sales_sub = (\n",
    "    sales[sales[\"id\"].str.contains(\"|\".join(depts)) & sales[\"id\"].str.contains(\"|\".join(stores))]\n",
    "    .groupby(\"id\")\n",
    "    .head(400)   # grab ~400 days per series\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Drop zero-sales rows to simulate missingness (for teaching padding)\n",
    "sales = sales[sales[\"sales\"] > 0]\n",
    "sales_sub = sales_sub[sales_sub[\"sales\"] > 0]\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Train/test splits\n",
    "train_full, test_full = make_train_test_split(sales, date_col=\"date\", horizon=28)\n",
    "train_sub, test_sub = make_train_test_split(sales_sub, date_col=\"date\", horizon=28)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Save outputs\n",
    "\n",
    "# Sales splits\n",
    "train_full.to_parquet(INPUT_PROCESSED / \"m5_sales_train_full.parquet\", index=False)\n",
    "test_full.to_parquet(INPUT_PROCESSED / \"m5_sales_test_full.parquet\", index=False)\n",
    "\n",
    "train_sub.to_parquet(INPUT_PROCESSED / \"m5_sales_train_subset.parquet\", index=False)\n",
    "test_sub.to_parquet(INPUT_PROCESSED / \"m5_sales_test_subset.parquet\", index=False)\n",
    "\n",
    "# Calendar & prices (save only full since they are known)\n",
    "calendar.to_parquet(INPUT_PROCESSED / \"m5_calendar_full.parquet\", index=False)\n",
    "prices.to_parquet(INPUT_PROCESSED / \"m5_prices_full.parquet\", index=False)\n",
    "\n",
    "print(\"âœ… Training/test sets saved:\")\n",
    "print(\" - Sales (full + subset)\")\n",
    "print(\" - Calendar (full only)\")\n",
    "print(\" - Prices (full only)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673e5d42",
   "metadata": {},
   "source": [
    "### 6) Quick sanity checks\n",
    "\n",
    "- Confirm columns exist and data spans multiple years  \n",
    "- Standardize an **ID** column for later notebooks (e.g., `id = item_id + '_' + store_id`)  \n",
    "- Identify the **value column** (`sales`, `value`, etc.) robustly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f8cd5e",
   "metadata": {},
   "source": [
    "### 6) Quick sanity checks\n",
    "\n",
    "Weâ€™ll run a few checks to make sure the train/test splits are valid:\n",
    "\n",
    "- **Confirm columns exist** and cover the expected date ranges  \n",
    "- **Standardize an ID column** (`id = item_id + '_' + store_id`) if not already present  \n",
    "- **Identify the value column** (`sales`, `value`, etc.) robustly  \n",
    "- **Check consistency between train and test**:  \n",
    "  - Train end date and test start date are exactly 1 day apart  \n",
    "  - All series IDs in test are also present in train  \n",
    "  - The number of unique series is consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "deb2cdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset TRAIN rows: 565353 | Date range: 2011-01-29 00:00:00 â†’ 2016-05-22 00:00:00\n",
      "Subset TEST rows: 170 | Date range: 2016-05-23 00:00:00 â†’ 2016-06-19 00:00:00\n",
      "Calendar full cols: ['unique_id', 'date', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX'] ...\n",
      "Prices full cols: ['unique_id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'] ...\n",
      "Value column detected: sales\n",
      "\n",
      "Consistency checks:\n",
      " - Train last date: 2016-05-22 00:00:00\n",
      " - Test first date: 2016-05-23 00:00:00\n",
      " - Gap between train end and test start: 1 days\n",
      " - All test IDs in train? True\n",
      " - # unique IDs in train: 2710 | in test: 13\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "INPUT_PROCESSED = Path(\"..\") / \"data\" / \"input\" / \"processed\"\n",
    "\n",
    "# Load subset train/test\n",
    "train_sub = pd.read_parquet(INPUT_PROCESSED / \"m5_sales_train_subset.parquet\")\n",
    "test_sub = pd.read_parquet(INPUT_PROCESSED / \"m5_sales_test_subset.parquet\")\n",
    "\n",
    "# Load calendar & prices (full only)\n",
    "calendar_full = pd.read_parquet(INPUT_PROCESSED / \"m5_calendar_full.parquet\")\n",
    "prices_full = pd.read_parquet(INPUT_PROCESSED / \"m5_prices_full.parquet\")\n",
    "\n",
    "# Ensure stable ID\n",
    "if \"id\" not in train_sub.columns:\n",
    "    train_sub[\"id\"] = train_sub[\"item_id\"].astype(str) + \"_\" + train_sub[\"store_id\"].astype(str)\n",
    "    test_sub[\"id\"] = test_sub[\"item_id\"].astype(str) + \"_\" + test_sub[\"store_id\"].astype(str)\n",
    "\n",
    "# Detect value column\n",
    "possible_vals = [\"sales\",\"value\",\"demand\",\"qty\",\"units\"]\n",
    "value_col = next((c for c in possible_vals if c in train_sub.columns), None)\n",
    "if value_col is None:\n",
    "    raise ValueError(f\"Couldn't find a value column in {possible_vals}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Quick info\n",
    "print(\"Subset TRAIN rows:\", len(train_sub), \n",
    "      \"| Date range:\", train_sub[\"date\"].min(), \"â†’\", train_sub[\"date\"].max())\n",
    "print(\"Subset TEST rows:\", len(test_sub), \n",
    "      \"| Date range:\", test_sub[\"date\"].min(), \"â†’\", test_sub[\"date\"].max())\n",
    "print(\"Calendar full cols:\", list(calendar_full.columns)[:8], \"...\")\n",
    "print(\"Prices full cols:\", list(prices_full.columns)[:8], \"...\")\n",
    "print(\"Value column detected:\", value_col)\n",
    "\n",
    "# -------------------------------\n",
    "# Consistency checks\n",
    "train_end = train_sub[\"date\"].max()\n",
    "test_start = test_sub[\"date\"].min()\n",
    "\n",
    "ids_train = set(train_sub[\"id\"].unique())\n",
    "ids_test = set(test_sub[\"id\"].unique())\n",
    "\n",
    "print(\"\\nConsistency checks:\")\n",
    "print(\" - Train last date:\", train_end)\n",
    "print(\" - Test first date:\", test_start)\n",
    "print(\" - Gap between train end and test start:\", (test_start - train_end).days, \"days\")\n",
    "print(\" - All test IDs in train?\", ids_test.issubset(ids_train))\n",
    "print(\" - # unique IDs in train:\", len(ids_train), \"| in test:\", len(ids_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec00ccc",
   "metadata": {},
   "source": [
    "### 7) (Optional) .gitignore helper\n",
    "\n",
    "If youâ€™re versioning this repo, itâ€™s smart to ignore **raw data** and **outputs**.  \n",
    "Weâ€™ll generate a starter `.gitignore` so large files donâ€™t accidentally end up in git history. Run this cell once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307ad5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gitignore_path = ROOT / \".gitignore\"\n",
    "lines = [\n",
    "    \"# data (raw, interim, output)\",\n",
    "    \"data/input/raw/\",\n",
    "    \"data/interim/\",\n",
    "    \"data/output/\",\n",
    "    \"\",\n",
    "    \"# OS/editor files\",\n",
    "    \".DS_Store\",\n",
    "    \".ipynb_checkpoints/\",\n",
    "    \"\"\n",
    "]\n",
    "\n",
    "if not gitignore_path.exists():\n",
    "    gitignore_path.write_text(\"\\n\".join(lines))\n",
    "    print(\"Created .gitignore with data folders ignored.\")\n",
    "else:\n",
    "    print(\".gitignore already exists â€” review to ensure data folders are ignored.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e46bb70",
   "metadata": {},
   "source": [
    "### Youâ€™re set ðŸŽ‰\n",
    "\n",
    "- **Raw M5** lives in `data/input/raw/` (Parquet).  \n",
    "- **Teaching subset** lives in `data/input/processed/`.  \n",
    "- Optional diagnostics show the data is usable at **weekly & monthly grains**.  \n",
    "\n",
    "**Next:** move to `01_framing.ipynb` to define the forecast charter (target, grain, horizon, metrics) and start exploring the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddc7c32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsforge-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
